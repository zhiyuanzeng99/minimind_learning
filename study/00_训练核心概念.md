# 00 - 深度学习训练核心概念

在阅读训练代码之前，先理解这些基本概念。

---

## 一、数据相关

### 1.1 Epoch（轮次）

```
1 Epoch = 把所有训练数据完整过一遍

例如：数据集有 10000 条
- 1 epoch = 模型看了 10000 条数据
- 3 epochs = 模型看了 30000 条数据（同样的数据看3遍）
```

**选择建议**：
| 阶段 | 建议 epochs | 原因 |
|------|-------------|------|
| Pretrain | 1-2 | 数据量大，1轮就够 |
| SFT | 2-5 | 数据量中等 |
| LoRA | 10-50 | 数据量小，需要多轮 |
| DPO | 1-2 | 防止过拟合 |

---

### 1.2 Batch Size（批大小）

```
每次更新参数时，同时处理多少条数据

batch_size = 32 意味着：
- 一次前向传播处理 32 条数据
- 计算 32 条数据的平均 loss
- 做一次反向传播更新参数
```

**batch_size 的影响**：

| batch_size | 优点 | 缺点 |
|------------|------|------|
| 大 (64-256) | 训练稳定、GPU利用率高、梯度估计准确 | 显存占用大 |
| 小 (8-32) | 显存占用小、有正则化效果 | 训练不稳定、速度慢 |

**RTX 4090 (24GB) 建议**：
| 阶段 | batch_size |
|------|------------|
| Pretrain | 64-128 |
| SFT | 32-64 |
| LoRA | 32-64 |
| DPO | 8-16 |

---

### 1.3 Steps / Iterations（步数）

```
数据集 10000 条，batch_size = 100

1 epoch 需要多少 steps？
steps = 10000 / 100 = 100 steps

总 steps = steps_per_epoch × epochs
```

**关系图**：
```
数据集大小: 10000
batch_size: 100
                ↓
steps_per_epoch = 10000 / 100 = 100
                ↓
epochs = 2
                ↓
total_steps = 100 × 2 = 200
```

---

## 二、学习率相关

### 2.1 Learning Rate（学习率）

```
每次更新参数时，"迈多大的步子"

公式：新参数 = 旧参数 - learning_rate × 梯度
```

**直观理解**：
```
学习率大 = 步子大 = 学得快但可能跳过最优解
学习率小 = 步子小 = 学得慢但更精确
```

**各阶段建议**：
| 阶段 | 学习率 | 原因 |
|------|--------|------|
| Pretrain | 5e-4 (0.0005) | 从头学习，步子可以大 |
| SFT | 1e-6 (0.000001) | 微调，步子小防止遗忘 |
| LoRA | 1e-4 (0.0001) | 只调小矩阵，可以稍大 |
| DPO | 4e-8 | 对齐阶段，非常小 |
| PPO | 8e-8 | 强化学习，非常小 |

**学习率过大 vs 过小**：
```
过大：
- loss 震荡不下降
- 出现 NaN
- 模型"发疯"

过小：
- 收敛极慢
- 训练时间太长
- 可能卡在局部最优
```

---

### 2.2 Learning Rate Schedule（学习率调度）

训练过程中动态调整学习率。

**MiniMind 使用的余弦退火**：
```python
def get_lr(current_step, total_steps, lr):
    return lr * (0.1 + 0.45 * (1 + cos(π * current_step / total_steps)))
```

**图示**：
```
lr
 ^
 |  ____
 | /    \
 |/      \____
 +-------------> step
   开始高    结束低
```

**为什么要调度**：
- 开始用大学习率，快速学习
- 后期用小学习率，精细调整

---

### 2.3 Gradient Accumulation（梯度累积）

**问题**：想用大 batch_size，但显存不够

**解决**：累积多个小 batch 的梯度，再更新

```python
accumulation_steps = 8
batch_size = 32
等效 batch_size = 32 × 8 = 256

实际过程：
step 1: 计算梯度，累积
step 2: 计算梯度，累积
...
step 8: 计算梯度，累积，更新参数，清零
```

**代码示例**：
```python
for step, (x, y) in enumerate(loader):
    loss = model(x, y).loss
    loss = loss / accumulation_steps  # 平均
    loss.backward()  # 累积梯度

    if (step + 1) % accumulation_steps == 0:
        optimizer.step()  # 更新参数
        optimizer.zero_grad()  # 清零梯度
```

---

### 2.4 Gradient Clipping（梯度裁剪）

**问题**：梯度爆炸导致训练不稳定

**解决**：限制梯度的最大值

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 如果梯度的模 > 1.0，就缩放到 1.0
```

**什么时候用**：
- 训练 Transformer 时几乎必用
- 出现 NaN 时可以尝试减小 max_norm

---

## 三、序列相关

### 3.1 Max Sequence Length（最大序列长度）

```
每条数据最多处理多少个 token

max_seq_len = 512 意味着：
- 超过 512 token 的文本会被截断
- 不足 512 的会用 pad token 填充
```

**中文 token 数估算**：
```
中文：1 token ≈ 1.5 字
max_seq_len = 512 ≈ 340 个汉字
```

**显存影响**：
```
Attention 显存 ∝ seq_len²

seq_len = 512  → 显存 X
seq_len = 1024 → 显存 4X
seq_len = 2048 → 显存 16X
```

---

## 四、保存相关

### 4.1 Save Interval（保存间隔）

```python
--save_interval 1000

# 每 1000 步保存一次模型
# 第 1000 步保存
# 第 2000 步保存
# ...
```

**建议**：
- 训练时间长：500-1000 步保存一次
- 训练时间短：训练结束时保存

### 4.2 Log Interval（日志间隔）

```python
--log_interval 100

# 每 100 步打印一次日志
# 显示 loss、学习率、预计时间等
```

---

## 五、混合精度相关

### 5.1 dtype（数据类型）

```python
--dtype bfloat16  # 或 float16
```

| 类型 | 精度 | 动态范围 | 特点 |
|------|------|----------|------|
| float32 | 高 | 大 | 默认，显存占用大 |
| float16 | 中 | 小 | 需要 GradScaler |
| bfloat16 | 低 | 大 | 推荐，稳定不需要 Scaler |

**为什么用混合精度**：
- 显存减半
- 训练加速
- 精度损失很小

---

## 六、分布式相关

### 6.1 多卡训练

```bash
# 单卡
python train.py --batch_size 64

# 4卡 (使用 torchrun)
torchrun --nproc_per_node=4 train.py --batch_size 64
# 等效 batch_size = 64 × 4 = 256
```

**数据并行原理**：
```
4 张 GPU，batch_size = 64

每张 GPU:
- 拿到 64 条不同的数据
- 各自计算梯度
- 梯度同步（平均）
- 各自更新参数（参数相同）

总处理量 = 64 × 4 = 256 条/step
```

---

## 七、参数关系总结

```
                        数据集大小 (N)
                              ↓
          Steps per Epoch = N / batch_size
                              ↓
          Total Steps = Steps per Epoch × Epochs
                              ↓
                     每 step 更新一次参数
                              ↓
              参数更新量 = learning_rate × 梯度
```

---

## 八、调参口诀

```
显存不够 → 减 batch_size 或 max_seq_len 或 用梯度累积
训练不稳 → 减 learning_rate 或 加 grad_clip
训练太慢 → 加 batch_size 或 多卡并行
Loss 不降 → 检查数据、调整 learning_rate
出现 NaN → 减 learning_rate、加 grad_clip、换 bfloat16
过拟合了 → 减 epochs 或 加 dropout 或 加数据
```

---

## 九、MiniMind 默认参数速查

```python
# train_pretrain.py 默认参数
--epochs 1
--batch_size 32
--learning_rate 5e-4
--accumulation_steps 8      # 等效 batch = 256
--grad_clip 1.0
--max_seq_len 340
--hidden_size 512
--num_hidden_layers 8
--save_interval 1000
--log_interval 100
--dtype bfloat16
```

---

## 十、实际训练时观察什么

### Loss 曲线
```
正常：稳定下降，偶尔小波动
异常：
- 不下降 → 学习率太小
- 剧烈震荡 → 学习率太大
- 突然上升 → 可能过拟合或数据问题
```

### 显存占用
```bash
# 实时查看
watch -n 1 nvidia-smi
```

### 训练速度
```
关注 steps/sec 或 samples/sec
速度太慢可能是：
- 数据加载瓶颈（增加 num_workers）
- batch_size 太小
- 没用混合精度
```
