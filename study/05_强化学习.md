# 05 - 强化学习详解 (DPO & PPO)

## 核心文件
- `trainer/train_dpo.py` (218行) - DPO训练
- `trainer/train_ppo.py` (380行) - PPO训练

---

## 一、为什么需要强化学习

### 1.1 SFT的局限性

SFT后的模型能对话了，但可能：
- 回答不够有帮助
- 有时不礼貌
- 可能生成有害内容
- 不符合人类偏好

### 1.2 强化学习的目标

让模型学会：
- 什么样的回答是**好的**
- 什么样的回答是**不好的**
- 如何生成**人类更喜欢**的回答

### 1.3 两种主流方法

| 方法 | 全称 | 特点 |
|------|------|------|
| **DPO** | Direct Preference Optimization | 简单，不需要奖励模型 |
| **PPO** | Proximal Policy Optimization | 经典，需要奖励模型 |

---

## 二、DPO 直接偏好优化

### 2.1 DPO的原理

**核心思想**：直接从偏好数据学习，不需要训练奖励模型

```
输入: 同一个问题的两个回答
- chosen (好的回答): 礼貌、准确、有帮助
- rejected (坏的回答): 粗鲁、错误、无用

目标: 让模型更倾向于生成chosen那样的回答
```

### 2.2 DPO损失函数

**位置**: `train_dpo.py` 第33-51行

```python
def dpo_loss(ref_log_probs, policy_log_probs, mask, beta):
    # 计算每个序列的平均log概率
    seq_lengths = mask.sum(dim=1, keepdim=True).clamp_min(1e-8)
    ref_log_probs = (ref_log_probs * mask).sum(dim=1) / seq_lengths.squeeze()
    policy_log_probs = (policy_log_probs * mask).sum(dim=1) / seq_lengths.squeeze()

    # 分离chosen和rejected
    batch_size = ref_log_probs.shape[0]
    chosen_ref_log_probs = ref_log_probs[:batch_size // 2]
    reject_ref_log_probs = ref_log_probs[batch_size // 2:]
    chosen_policy_log_probs = policy_log_probs[:batch_size // 2]
    reject_policy_log_probs = policy_log_probs[batch_size // 2:]

    # DPO核心公式
    pi_logratios = chosen_policy_log_probs - reject_policy_log_probs
    ref_logratios = chosen_ref_log_probs - reject_ref_log_probs
    logits = pi_logratios - ref_logratios

    # 最终损失
    loss = -F.logsigmoid(beta * logits)
    return loss.mean()
```

**公式解释**：
```
L_DPO = -log(σ(β × (log π(chosen)/π(rejected) - log π_ref(chosen)/π_ref(rejected))))

其中：
- π: 当前策略模型
- π_ref: 参考模型（冻结的SFT模型）
- β: 温度参数，控制偏好强度
- σ: sigmoid函数
```

### 2.3 DPO数据格式

```json
{
  "chosen": [
    {"role": "user", "content": "你好"},
    {"role": "assistant", "content": "你好！很高兴见到你，有什么可以帮助你的吗？"}
  ],
  "rejected": [
    {"role": "user", "content": "你好"},
    {"role": "assistant", "content": "嗯。"}
  ]
}
```

### 2.4 DPO训练流程

```python
# 1. 加载策略模型和参考模型
model, tokenizer = init_model(lm_config, 'full_sft')      # 策略模型
ref_model, _ = init_model(lm_config, 'full_sft')          # 参考模型
ref_model.eval()
ref_model.requires_grad_(False)  # 冻结参考模型

# 2. 训练循环
for batch in loader:
    x_chosen, x_rejected = batch['x_chosen'], batch['x_rejected']
    y_chosen, y_rejected = batch['y_chosen'], batch['y_rejected']

    # 拼接chosen和rejected
    x = torch.cat([x_chosen, x_rejected], dim=0)
    y = torch.cat([y_chosen, y_rejected], dim=0)

    # 计算参考模型的log概率（不更新梯度）
    with torch.no_grad():
        ref_logits = ref_model(x).logits
        ref_log_probs = logits_to_log_probs(ref_logits, y)

    # 计算策略模型的log概率
    policy_logits = model(x).logits
    policy_log_probs = logits_to_log_probs(policy_logits, y)

    # 计算DPO损失
    loss = dpo_loss(ref_log_probs, policy_log_probs, mask, beta=0.1)

    loss.backward()
    optimizer.step()
```

### 2.5 DPO参数

```python
parser.add_argument("--learning_rate", default=4e-8)  # 非常小的学习率
parser.add_argument("--batch_size", default=4)
parser.add_argument('--beta', default=0.1)            # DPO温度参数
parser.add_argument('--from_weight', default='full_sft')
```

**关键参数**：
| 参数 | 默认值 | 说明 |
|------|--------|------|
| `learning_rate` | 4e-8 | 非常小，防止遗忘 |
| `beta` | 0.1 | 偏好强度，越大偏好越强 |

---

## 三、PPO 近端策略优化

### 3.1 PPO的原理

PPO是经典的强化学习算法：

```
1. 策略模型生成回答
2. 奖励模型给回答打分
3. 用PPO算法更新策略模型
   - 高分回答 → 增加生成概率
   - 低分回答 → 降低生成概率
```

### 3.2 PPO的组件

**位置**: `train_ppo.py`

```python
# Actor模型：生成回答
actor_model, tokenizer = init_model(lm_config, 'full_sft')

# Old Actor模型：计算重要性采样比率
old_actor_model, _ = init_model(lm_config, 'full_sft')
old_actor_model.eval().requires_grad_(False)

# Reference模型：计算KL惩罚
ref_model, _ = init_model(lm_config, 'full_sft')
ref_model.eval().requires_grad_(False)

# Critic模型：估计状态价值
class CriticModel(MiniMindForCausalLM):
    def __init__(self, params):
        super().__init__(params)
        self.value_head = nn.Linear(params.hidden_size, 1)  # 输出价值

    def forward(self, input_ids, attention_mask):
        hidden_states = self.model(input_ids, attention_mask)[0]
        values = self.value_head(hidden_states).squeeze(-1)
        return values

# Reward模型：给回答打分（外部模型）
reward_model = AutoModel.from_pretrained('reward_model_path')
```

### 3.3 奖励计算

```python
def calculate_rewards(prompts, responses, reward_model, reward_tokenizer):
    rewards = torch.zeros(len(responses))

    # 格式奖励（推理模型用）
    if args.reasoning == 1:
        for i, response in enumerate(responses):
            if '<think>' in response and '</think>' in response:
                rewards[i] += 0.5  # 格式正确奖励

    # 奖励模型打分
    for i, (prompt, response) in enumerate(zip(prompts, responses)):
        score = reward_model.get_score(prompt + response)
        score = max(min(score, 3.0), -3.0)  # 裁剪到[-3, 3]
        rewards[i] += score

    return rewards
```

### 3.4 PPO损失函数

```python
# 计算优势函数
advantages = rewards - values.detach()

# 计算策略比率
ratio = torch.exp(actor_logp - old_logp)

# PPO裁剪损失
surr1 = ratio * advantages
surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages
policy_loss = -torch.min(surr1, surr2).mean()

# 价值损失
value_loss = F.mse_loss(values, rewards)

# KL惩罚
kl_penalty = (actor_logp - ref_logp).mean()

# 总损失
loss = policy_loss + vf_coef * value_loss + kl_coef * kl_penalty
```

**PPO核心思想**：限制每次更新的幅度

```
如果 ratio 偏离1太多（说明策略变化太大），就裁剪掉
这样可以防止策略"跑偏"
```

### 3.5 PPO参数

```python
parser.add_argument("--learning_rate", default=8e-8)
parser.add_argument("--critic_learning_rate", default=8e-8)
parser.add_argument("--clip_epsilon", default=0.1)      # PPO裁剪参数
parser.add_argument("--vf_coef", default=0.5)           # 价值损失系数
parser.add_argument("--kl_coef", default=0.02)          # KL惩罚系数
parser.add_argument("--max_gen_len", default=1536)      # 生成最大长度
```

---

## 四、DPO vs PPO 对比

| 对比项 | DPO | PPO |
|--------|-----|-----|
| **复杂度** | 简单 | 复杂 |
| **需要奖励模型** | 不需要 | 需要 |
| **训练稳定性** | 稳定 | 较难调 |
| **内存占用** | 2个模型 | 4-5个模型 |
| **数据要求** | 偏好对比数据 | prompt + 奖励模型 |
| **效果** | 好 | 可能更好 |
| **适用场景** | 有偏好数据时 | 有奖励模型时 |

---

## 五、运行示例

### 5.1 DPO训练

```bash
python trainer/train_dpo.py \
    --epochs 1 \
    --batch_size 4 \
    --learning_rate 4e-8 \
    --beta 0.1 \
    --from_weight full_sft \
    --data_path ../dataset/dpo.jsonl
```

### 5.2 PPO训练

```bash
python trainer/train_ppo.py \
    --epochs 1 \
    --batch_size 2 \
    --learning_rate 8e-8 \
    --clip_epsilon 0.1 \
    --kl_coef 0.02 \
    --reward_model_path path/to/reward_model \
    --data_path ../dataset/rlaif-mini.jsonl
```

---

## 六、数据准备

### 6.1 DPO数据格式

```json
{
  "chosen": [
    {"role": "user", "content": "帮我写一首诗"},
    {"role": "assistant", "content": "春风拂面暖阳照，\n柳絮飘飘似雪飘。\n..."}
  ],
  "rejected": [
    {"role": "user", "content": "帮我写一首诗"},
    {"role": "assistant", "content": "好的。诗。"}
  ]
}
```

### 6.2 PPO数据格式

```json
{"prompt": "<|im_start|>user\n请解释什么是机器学习<|im_end|>\n<|im_start|>assistant\n"}
```

---

## 七、训练技巧

### 7.1 DPO技巧

1. **学习率要小**：4e-8 ~ 1e-7
2. **beta不要太大**：0.1 ~ 0.5
3. **数据质量很重要**：chosen和rejected差异要明显

### 7.2 PPO技巧

1. **KL惩罚很重要**：防止模型偏离太远
2. **奖励裁剪**：防止极端奖励
3. **定期更新old_actor**：保持策略稳定
4. **Critic要同步训练**：价值估计要准确

---

## 八、常见问题

### Q1: DPO效果不好？
- 检查chosen和rejected的差异是否明显
- 尝试调整beta
- 增加数据量

### Q2: PPO训练不稳定？
- 减小学习率
- 增大KL惩罚系数
- 检查奖励模型质量

### Q3: 没有奖励模型怎么办？
- 使用DPO（不需要奖励模型）
- 使用规则奖励（如格式检查）
- 使用现成的开源奖励模型

---

## 九、输出文件

```
out/
├── dpo_512.pth           # DPO训练后的模型
├── ppo_actor_512.pth     # PPO训练后的Actor模型
└── checkpoints/
    ├── dpo_512_resume.pth
    └── ppo_actor_512_resume.pth
```
