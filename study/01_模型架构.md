# 01 - 模型架构详解

## 核心文件
- `model/model_minimind.py` (463行)

---

## 一、整体架构

MiniMind 是一个 **Decoder-Only Transformer**，和 GPT、LLaMA 架构类似：

```
输入 token_ids
    ↓
[Embedding] 把token转成向量
    ↓
[Transformer Block × 8] 核心计算层
    ↓
[RMSNorm] 最终归一化
    ↓
[LM Head] 输出词表概率
    ↓
输出 logits
```

---

## 二、模型配置 MiniMindConfig

**位置**: `model/model_minimind.py` 第8-79行

```python
class MiniMindConfig(PretrainedConfig):
    def __init__(
        self,
        hidden_size: int = 512,        # 隐藏层维度
        num_hidden_layers: int = 8,    # Transformer层数
        num_attention_heads: int = 8,  # 注意力头数
        num_key_value_heads: int = 2,  # KV头数（GQA）
        vocab_size: int = 6400,        # 词表大小
        max_position_embeddings: int = 32768,  # 最大位置
        rope_theta: int = 1000000.0,   # RoPE基数
        ...
    )
```

### 关键参数解释

| 参数 | 默认值 | 作用 |
|------|--------|------|
| `hidden_size` | 512 | 每个token的向量维度 |
| `num_hidden_layers` | 8 | 有多少个Transformer块 |
| `num_attention_heads` | 8 | 注意力分成几个头 |
| `num_key_value_heads` | 2 | KV缓存的头数（GQA优化）|
| `vocab_size` | 6400 | 词表有多少个词 |
| `intermediate_size` | hidden_size*8/3 | FFN中间层大小 |
| `dropout` | 0.0 | Dropout概率 |
| `flash_attn` | True | 是否用Flash Attention |

### 模型规模对比

| 配置 | hidden_size | layers | 参数量 |
|------|-------------|--------|--------|
| Small | 512 | 8 | 26M |
| Base | 768 | 16 | 104M |
| MoE | 640 | 8 | 145M(激活25M) |

---

## 三、核心组件详解

### 3.1 RMSNorm (第96-106行)

**作用**：归一化层，比LayerNorm更高效

```python
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        # RMS = sqrt(mean(x^2))
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        return self.weight * self._norm(x.float()).type_as(x)
```

**原理**：
- LayerNorm: `(x - mean) / std`
- RMSNorm: `x / RMS(x)`，省去了计算均值的步骤

**为什么用RMSNorm**：
1. 计算更快（少一次mean计算）
2. 效果和LayerNorm差不多
3. LLaMA等模型都在用

---

### 3.2 RoPE 旋转位置编码 (第109-137行)

**作用**：让模型知道每个token的位置

```python
def precompute_freqs_cis(dim: int, end: int = int(32 * 1024),
                         rope_base: float = 1e6, rope_scaling=None):
    # 计算频率
    freqs = 1.0 / (rope_base ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))

    # 生成位置序列
    t = torch.arange(end, device=freqs.device)
    freqs = torch.outer(t, freqs).float()

    # 返回cos和sin
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1)
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1)
    return freqs_cos, freqs_sin
```

**原理**：
- 传统位置编码：把位置信息加到embedding上
- RoPE：通过旋转query和key来编码位置
- 公式：`q' = q * cos(θ) + rotate(q) * sin(θ)`

**为什么用RoPE**：
1. 可以外推到更长的序列
2. 相对位置信息自然融入注意力计算
3. 效果比传统位置编码好

---

### 3.3 Attention 自注意力 (第150-213行)

**作用**：让每个token能"看到"其他token

```python
class Attention(nn.Module):
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        # GQA: num_attention_heads=8, num_key_value_heads=2
        self.n_local_heads = args.num_attention_heads      # Q的头数: 8
        self.n_local_kv_heads = self.num_key_value_heads   # KV的头数: 2
        self.n_rep = self.n_local_heads // self.n_local_kv_heads  # 重复倍数: 4

        self.head_dim = args.hidden_size // args.num_attention_heads  # 64

        # 线性投影层
        self.q_proj = nn.Linear(hidden_size, num_heads * head_dim)      # 512 → 512
        self.k_proj = nn.Linear(hidden_size, num_kv_heads * head_dim)   # 512 → 128
        self.v_proj = nn.Linear(hidden_size, num_kv_heads * head_dim)   # 512 → 128
        self.o_proj = nn.Linear(num_heads * head_dim, hidden_size)      # 512 → 512
```

**GQA (Grouped Query Attention) 原理**：

```
标准MHA:  Q有8个头，K有8个头，V有8个头
GQA:      Q有8个头，K有2个头，V有2个头

Q的每4个头共享1组KV，节省显存
```

**Forward 流程**：

```python
def forward(self, x, position_embeddings, ...):
    # 1. 线性投影
    xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)

    # 2. reshape成多头
    xq = xq.view(bsz, seq_len, n_heads, head_dim)      # [B, L, 8, 64]
    xk = xk.view(bsz, seq_len, n_kv_heads, head_dim)   # [B, L, 2, 64]
    xv = xv.view(bsz, seq_len, n_kv_heads, head_dim)   # [B, L, 2, 64]

    # 3. 应用RoPE
    xq, xk = apply_rotary_pos_emb(xq, xk, cos, sin)

    # 4. KV缓存（推理时用）
    if past_key_value is not None:
        xk = torch.cat([past_key_value[0], xk], dim=1)
        xv = torch.cat([past_key_value[1], xv], dim=1)

    # 5. GQA: 把KV重复以匹配Q的头数
    xk = repeat_kv(xk, n_rep)  # [B, L, 2, 64] → [B, L, 8, 64]
    xv = repeat_kv(xv, n_rep)

    # 6. 计算注意力
    if self.flash:  # Flash Attention
        output = F.scaled_dot_product_attention(xq, xk, xv, is_causal=True)
    else:  # 标准注意力
        scores = (xq @ xk.transpose(-2, -1)) / sqrt(head_dim)
        scores = scores + causal_mask  # 因果mask
        scores = softmax(scores)
        output = scores @ xv

    # 7. 输出投影
    output = self.o_proj(output.reshape(bsz, seq_len, -1))
    return output
```

---

### 3.4 FeedForward 前馈网络 (第216-229行)

**作用**：非线性变换，增加模型容量

```python
class FeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        # 中间层大小 = hidden_size * 8/3，再对齐到64的倍数
        intermediate_size = int(config.hidden_size * 8 / 3)
        intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)

        self.gate_proj = nn.Linear(hidden_size, intermediate_size)  # 512 → 1088
        self.up_proj = nn.Linear(hidden_size, intermediate_size)    # 512 → 1088
        self.down_proj = nn.Linear(intermediate_size, hidden_size)  # 1088 → 512
        self.act_fn = SiLU()

    def forward(self, x):
        # SwiGLU激活: gate * up，然后down
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
```

**SwiGLU 原理**：
```
标准FFN: down(relu(up(x)))
SwiGLU:  down(silu(gate(x)) * up(x))
```
SwiGLU效果更好，LLaMA、Qwen等都在用。

---

### 3.5 MiniMindBlock (第352-373行)

**作用**：一个完整的Transformer层

```python
class MiniMindBlock(nn.Module):
    def __init__(self, layer_id: int, config: MiniMindConfig):
        super().__init__()
        self.self_attn = Attention(config)
        self.mlp = FeedForward(config)  # 或 MOEFeedForward
        self.input_layernorm = RMSNorm(config.hidden_size)
        self.post_attention_layernorm = RMSNorm(config.hidden_size)

    def forward(self, hidden_states, position_embeddings, ...):
        # Pre-Norm结构
        residual = hidden_states

        # 1. Attention
        hidden_states = self.self_attn(
            self.input_layernorm(hidden_states),  # 先norm
            position_embeddings
        )
        hidden_states = hidden_states + residual  # 残差连接

        # 2. FFN
        hidden_states = hidden_states + self.mlp(
            self.post_attention_layernorm(hidden_states)  # 先norm
        )

        return hidden_states
```

**Pre-Norm vs Post-Norm**：
```
Post-Norm: x + norm(attn(x))  # GPT-2用的
Pre-Norm:  x + attn(norm(x))  # LLaMA用的，训练更稳定
```

---

### 3.6 完整模型 MiniMindForCausalLM (第427-463行)

```python
class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    def __init__(self, config: MiniMindConfig):
        super().__init__(config)
        self.model = MiniMindModel(config)  # backbone
        self.lm_head = nn.Linear(hidden_size, vocab_size)  # 输出层

        # 权重共享：embedding和lm_head共用权重
        self.model.embed_tokens.weight = self.lm_head.weight

    def forward(self, input_ids, labels=None, ...):
        # 1. 过backbone
        hidden_states, past_kv, aux_loss = self.model(input_ids, ...)

        # 2. 输出logits
        logits = self.lm_head(hidden_states)  # [B, L, vocab_size]

        # 3. 计算loss（如果有labels）
        if labels is not None:
            shift_logits = logits[..., :-1, :]   # 预测位置
            shift_labels = labels[..., 1:]       # 目标位置（右移一位）
            loss = F.cross_entropy(shift_logits, shift_labels)

        return CausalLMOutputWithPast(loss=loss, logits=logits, ...)
```

---

## 四、MoE (混合专家) 架构

**位置**: 第232-349行

MoE的核心思想：**条件计算**，每个token只激活部分专家

### 4.1 MoEGate 门控 (第232-285行)

```python
class MoEGate(nn.Module):
    def __init__(self, config):
        self.top_k = config.num_experts_per_tok  # 每个token选2个专家
        self.n_routed_experts = config.n_routed_experts  # 总共4个专家
        self.weight = nn.Parameter(torch.empty((n_experts, hidden_size)))

    def forward(self, hidden_states):
        # 1. 计算每个专家的分数
        logits = F.linear(hidden_states, self.weight)  # [B*L, n_experts]
        scores = logits.softmax(dim=-1)

        # 2. 选top-k个专家
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k)

        # 3. 计算辅助损失（防止负载不均）
        aux_loss = ...

        return topk_idx, topk_weight, aux_loss
```

### 4.2 MOEFeedForward (第288-349行)

```python
class MOEFeedForward(nn.Module):
    def __init__(self, config):
        # 4个路由专家 + 1个共享专家
        self.experts = nn.ModuleList([FeedForward(config) for _ in range(4)])
        self.gate = MoEGate(config)
        self.shared_experts = nn.ModuleList([FeedForward(config)])

    def forward(self, x):
        # 1. 门控选择专家
        topk_idx, topk_weight, aux_loss = self.gate(x)

        # 2. 每个token只过选中的专家
        for i, expert in enumerate(self.experts):
            expert_out = expert(x[topk_idx == i])

        # 3. 加权求和
        y = (expert_out * topk_weight).sum()

        # 4. 加上共享专家的输出
        y = y + self.shared_experts[0](x)

        return y
```

**MoE的优点**：
- 参数量大但计算量小（145M参数，只激活25M）
- 每个专家可以学习不同的知识

---

## 五、代码阅读建议

1. **先看配置类** `MiniMindConfig`，了解有哪些参数
2. **看Forward流程**：`MiniMindForCausalLM.forward` → `MiniMindModel.forward` → `MiniMindBlock.forward`
3. **重点理解**：
   - Attention的GQA实现
   - RoPE位置编码
   - Pre-Norm残差连接

---

## 六、相关代码位置速查

| 组件 | 行号 | 关键方法 |
|------|------|----------|
| MiniMindConfig | 8-79 | `__init__` |
| RMSNorm | 96-106 | `forward`, `_norm` |
| RoPE | 109-137 | `precompute_freqs_cis`, `apply_rotary_pos_emb` |
| Attention | 150-213 | `forward` |
| FeedForward | 216-229 | `forward` |
| MoEGate | 232-285 | `forward` |
| MOEFeedForward | 288-349 | `forward`, `moe_infer` |
| MiniMindBlock | 352-373 | `forward` |
| MiniMindModel | 376-424 | `forward` |
| MiniMindForCausalLM | 427-463 | `forward` |
