# 06 - 模型保存与加载详解

## 核心文件
- `trainer/trainer_utils.py` (156行)
- 各训练脚本的保存逻辑

---

## 一、两种保存方式

MiniMind项目中有两种保存方式：

| 类型 | 文件名示例 | 用途 | 包含内容 |
|------|------------|------|----------|
| **权重保存** | `pretrain_512.pth` | 推理/继续训练 | 只有模型权重 |
| **断点保存** | `pretrain_512_resume.pth` | 断点续训 | 权重+优化器+状态 |

---

## 二、权重保存 (用于推理)

### 2.1 保存代码

**位置**: 各训练脚本的保存部分

```python
# 获取原始模型（去掉DDP和compile包装）
raw_model = model.module if isinstance(model, DistributedDataParallel) else model
raw_model = getattr(raw_model, '_orig_mod', raw_model)

# 获取状态字典
state_dict = raw_model.state_dict()

# 转换为半精度并保存
torch.save({k: v.half().cpu() for k, v in state_dict.items()},
           f'{save_dir}/pretrain_512.pth')
```

### 2.2 为什么转half()

```python
{k: v.half().cpu() for k, v in state_dict.items()}
```

- **half()**: 把float32转成float16，文件大小减半
- **cpu()**: 确保张量在CPU上，避免GPU显存问题

### 2.3 保存的内容

```python
# state_dict 的结构
{
    'model.embed_tokens.weight': tensor(...),        # Embedding
    'model.layers.0.self_attn.q_proj.weight': ...,  # 注意力层
    'model.layers.0.self_attn.k_proj.weight': ...,
    'model.layers.0.self_attn.v_proj.weight': ...,
    'model.layers.0.self_attn.o_proj.weight': ...,
    'model.layers.0.mlp.gate_proj.weight': ...,     # FFN层
    'model.layers.0.mlp.up_proj.weight': ...,
    'model.layers.0.mlp.down_proj.weight': ...,
    'model.layers.0.input_layernorm.weight': ...,   # LayerNorm
    ...
    'lm_head.weight': tensor(...),                   # 输出层
}
```

---

## 三、断点保存 (用于续训)

### 3.1 lm_checkpoint 函数

**位置**: `trainer/trainer_utils.py` 第63-116行

```python
def lm_checkpoint(lm_config, weight='full_sft', model=None, optimizer=None,
                  epoch=0, step=0, wandb=None, save_dir='../checkpoints', **kwargs):
    """
    保存模式：传入model参数
    加载模式：不传model参数
    """
    os.makedirs(save_dir, exist_ok=True)
    moe_path = '_moe' if lm_config.use_moe else ''
    ckp_path = f'{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}.pth'
    resume_path = f'{save_dir}/{weight}_{lm_config.hidden_size}{moe_path}_resume.pth'

    if model is not None:  # 保存模式
        # 获取原始模型
        raw_model = model.module if isinstance(model, DistributedDataParallel) else model
        raw_model = getattr(raw_model, '_orig_mod', raw_model)
        state_dict = raw_model.state_dict()
        state_dict = {k: v.half().cpu() for k, v in state_dict.items()}

        # 保存权重（使用临时文件防止写入中断）
        ckp_tmp = ckp_path + '.tmp'
        torch.save(state_dict, ckp_tmp)
        os.replace(ckp_tmp, ckp_path)

        # 获取wandb_id（用于续训时恢复wandb）
        wandb_id = None
        if wandb:
            if hasattr(wandb, 'get_run'):
                run = wandb.get_run()
                wandb_id = getattr(run, 'id', None) if run else None

        # 保存完整断点
        resume_data = {
            'model': state_dict,
            'optimizer': optimizer.state_dict(),
            'epoch': epoch,
            'step': step,
            'world_size': dist.get_world_size() if dist.is_initialized() else 1,
            'wandb_id': wandb_id
        }

        # 保存额外的状态（如scaler, scheduler等）
        for key, value in kwargs.items():
            if value is not None:
                if hasattr(value, 'state_dict'):
                    resume_data[key] = value.state_dict()
                else:
                    resume_data[key] = value

        resume_tmp = resume_path + '.tmp'
        torch.save(resume_data, resume_tmp)
        os.replace(resume_tmp, resume_path)

    else:  # 加载模式
        if os.path.exists(resume_path):
            ckp_data = torch.load(resume_path, map_location='cpu')

            # 处理GPU数量变化
            saved_ws = ckp_data.get('world_size', 1)
            current_ws = dist.get_world_size() if dist.is_initialized() else 1
            if saved_ws != current_ws:
                ckp_data['step'] = ckp_data['step'] * saved_ws // current_ws
                Logger(f'GPU数量变化({saved_ws}→{current_ws})，step已自动转换')

            return ckp_data
        return None
```

### 3.2 断点文件内容

```python
# resume.pth 的结构
{
    'model': {...},           # 模型权重
    'optimizer': {...},       # 优化器状态
    'scaler': {...},          # GradScaler状态
    'epoch': 0,               # 当前轮次
    'step': 1500,             # 当前步数
    'world_size': 4,          # 保存时的GPU数量
    'wandb_id': 'abc123',     # wandb运行ID
}
```

### 3.3 原子写入

```python
# 先写入临时文件
resume_tmp = resume_path + '.tmp'
torch.save(resume_data, resume_tmp)

# 再原子替换
os.replace(resume_tmp, resume_path)
```

**为什么这样做**：防止写入过程中断导致文件损坏

---

## 四、模型加载

### 4.1 init_model 函数

**位置**: `trainer/trainer_utils.py` 第119-131行

```python
def init_model(lm_config, from_weight='pretrain', tokenizer_path='../model',
               save_dir='../out', device='cuda'):
    # 加载分词器
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

    # 创建模型
    model = MiniMindForCausalLM(lm_config)

    # 加载权重（如果指定）
    if from_weight != 'none':
        moe_suffix = '_moe' if lm_config.use_moe else ''
        weight_path = f'{save_dir}/{from_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
        weights = torch.load(weight_path, map_location=device)
        model.load_state_dict(weights, strict=False)

    # 打印参数量
    get_model_params(model, lm_config)
    Logger(f'Trainable Params: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f}M')

    return model.to(device), tokenizer
```

### 4.2 strict=False 的作用

```python
model.load_state_dict(weights, strict=False)
```

- **strict=True**（默认）：权重必须完全匹配，否则报错
- **strict=False**：允许部分匹配，缺失的用随机初始化

**使用场景**：
- 加载预训练权重时，可能有些层不匹配（如LoRA层）
- MoE模型和普通模型切换时

### 4.3 断点续训加载

```python
# 检查是否有断点
ckp_data = lm_checkpoint(lm_config, weight='pretrain', save_dir='../checkpoints') \
           if args.from_resume == 1 else None

# 如果有断点，恢复状态
if ckp_data:
    model.load_state_dict(ckp_data['model'])
    optimizer.load_state_dict(ckp_data['optimizer'])
    scaler.load_state_dict(ckp_data['scaler'])
    start_epoch = ckp_data['epoch']
    start_step = ckp_data['step']
```

---

## 五、LoRA的保存与加载

### 5.1 LoRA保存

```python
def save_lora(model, path):
    raw_model = getattr(model, '_orig_mod', model)
    state_dict = {}

    for name, module in raw_model.named_modules():
        if hasattr(module, 'lora'):
            lora_state = {f'{name}.lora.{k}': v
                         for k, v in module.lora.state_dict().items()}
            state_dict.update(lora_state)

    torch.save(state_dict, path)
```

**保存的内容**：只有LoRA的A和B矩阵

```python
{
    'model.layers.0.self_attn.q_proj.lora.A.weight': tensor(...),
    'model.layers.0.self_attn.q_proj.lora.B.weight': tensor(...),
    ...
}
```

### 5.2 LoRA加载

```python
def load_lora(model, path):
    state_dict = torch.load(path, map_location=model.device)
    # 去掉'module.'前缀
    state_dict = {(k[7:] if k.startswith('module.') else k): v
                 for k, v in state_dict.items()}

    for name, module in model.named_modules():
        if hasattr(module, 'lora'):
            lora_state = {k.replace(f'{name}.lora.', ''): v
                         for k, v in state_dict.items() if f'{name}.lora.' in k}
            module.lora.load_state_dict(lora_state)
```

---

## 六、推理时的加载

### 6.1 eval_llm.py 中的加载

```python
def init_model(args):
    lm_config = MiniMindConfig(hidden_size=args.hidden_size, ...)

    # 加载基础模型
    model = MiniMindForCausalLM(lm_config)
    weight_path = f'out/{args.weight}_{args.hidden_size}.pth'
    weights = torch.load(weight_path, map_location=args.device)
    model.load_state_dict(weights, strict=False)

    # 如果有LoRA，加载LoRA
    if args.lora_weight:
        from model.model_lora import apply_lora, load_lora
        apply_lora(model, rank=8)
        lora_path = f'out/lora/{args.lora_weight}_{args.hidden_size}.pth'
        load_lora(model, lora_path)

    model.eval()  # 推理模式
    return model, tokenizer
```

---

## 七、文件命名规范

```
out/
├── pretrain_512.pth          # 预训练权重 (Small)
├── pretrain_768.pth          # 预训练权重 (Base)
├── pretrain_640_moe.pth      # 预训练权重 (MoE)
├── full_sft_512.pth          # SFT权重
├── dpo_512.pth               # DPO权重
├── ppo_actor_512.pth         # PPO权重
└── lora/
    ├── lora_identity_512.pth # 身份LoRA
    └── lora_medical_512.pth  # 医疗LoRA

checkpoints/
├── pretrain_512_resume.pth   # 预训练断点
├── full_sft_512_resume.pth   # SFT断点
└── dpo_512_resume.pth        # DPO断点
```

**命名规则**：`{类型}_{hidden_size}{_moe}.pth`

---

## 八、GPU数量变化处理

### 8.1 问题

用4卡训练到step 1000，想用2卡继续训练。

### 8.2 解决方案

`lm_checkpoint` 函数自动处理：

```python
saved_ws = ckp_data.get('world_size', 1)      # 保存时的GPU数量: 4
current_ws = dist.get_world_size()            # 当前GPU数量: 2

if saved_ws != current_ws:
    # 自动调整step
    ckp_data['step'] = ckp_data['step'] * saved_ws // current_ws
    # 1000 * 4 / 2 = 2000
```

**原理**：
- 4卡训练1000步 = 处理了 4000 个batch
- 2卡要处理同样多的数据 = 2000 步

---

## 九、常用操作

### 9.1 只加载部分权重

```python
# 加载预训练，但不加载lm_head
weights = torch.load('pretrain_512.pth')
del weights['lm_head.weight']
model.load_state_dict(weights, strict=False)
```

### 9.2 合并LoRA到基础模型

```python
# 加载基础模型
model = MiniMindForCausalLM(config)
model.load_state_dict(torch.load('full_sft_512.pth'))

# 应用并加载LoRA
apply_lora(model, rank=8)
load_lora(model, 'lora_identity_512.pth')

# 合并LoRA权重
for name, module in model.named_modules():
    if hasattr(module, 'lora'):
        # ΔW = B × A
        delta_w = module.lora.B.weight @ module.lora.A.weight
        module.weight.data += delta_w
        delattr(module, 'lora')

# 保存合并后的模型
torch.save(model.state_dict(), 'merged_model.pth')
```

### 9.3 查看权重内容

```python
weights = torch.load('pretrain_512.pth')
print(weights.keys())  # 查看所有键
print(weights['model.layers.0.self_attn.q_proj.weight'].shape)  # 查看某个权重的形状
```

---

## 十、注意事项

1. **保存时用临时文件**：防止中断导致文件损坏
2. **转half()节省空间**：推理够用，训练会自动转回float32
3. **strict=False要小心**：可能漏加载某些层
4. **断点要保存optimizer**：否则学习率会重置
5. **多卡训练注意world_size**：step数可能需要调整
